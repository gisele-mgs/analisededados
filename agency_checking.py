# -*- coding: utf-8 -*-
"""agency-checking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UvVdskcEuh2HHrLIIYZolsPNHso7l8Kq

# LDA + K-means: central de fatos

# 1. Importar base de dados e bibliotecas
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive/cp
!ls



import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
import ast
import re
import unicodedata
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import RSLPStemmer
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split

df_original = pd.read_csv( "/content/gdrive/MyDrive/cp/central_de_fatos.csv", decimal=',', header=0, sep=';')
df_original

"""## Atributos



1. `  url`: URL to the original fact-check instance;
2.  ` source_name:` Fact-checking agency;
3. `title:` Fact-check instance's title;
4. `subtitle:` Fact-check intance's subtitle;
5. `publication_date`: Publication date (YYYY-MM-DD format, where YYYY, MM and DD 6 represent year, month and day, respectively);
6. `text_news`: Fact-check instance's text body;
7. `image_link`: URL to image (if available);
8. `video_link`: URL to video (if available);
9. `authors`: Fact-check instance's author list;
10. `categories`: Categories associated with the fact-check instance by the agency;
10. `tags`: Keywords associated with the fact-check instance by the agency;
11. `obtained_at`: Date that data was collected (YYYY-MM-DD format);
12.` verdict label`: Each agency has its own set of labels, for instance true, false, fake, out of context,rumor, etc.

# 2.   Pré Processamento
"""

df_original.info()

"""Transformação da coluna `publication date` para o formato de **data**"""

df_original['publication_date'] = pd.to_datetime(
    df_original['publication_date'],
    format='%Y-%m-%d',  # Formato explícito para evitar ambiguidades
    errors='coerce'     # Converte datas inválidas em NaT (opcional)
)

# Verifique o resultado
df_original['publication_date'].head()

"""# 3. Seleção das colunas que irão ser analisadas"""

df_filtro = df_original[['url', 'source_name', 'title', 'publication_date', 'text_news','categories','tags', 'obtained_at','rating']]

df_filtro

"""# 4. Criando variável year, para auxiliar na analise"""

#df_filtro['publication_date'] = pd.to_datetime(df_filtro['publication_date'])

# Extrair o ano usando .dt.year
df_filtro['year'] = df_filtro['publication_date'].dt.year

"""# Filtrar os anos de eleições de 2014, 2016, 2018 e 2020"""

# Filtrar apenas os anos 2014, 2016, 2018 e 2020
anos_desejados = [2014, 2016, 2018, 2020]
df_filtro= df_filtro[df_filtro['year'].isin(anos_desejados)]
df_filtro

df_filtro.to_csv('/content/gdrive/MyDrive/df_filtro.csv', index=False)

df_filtro.describe()
df_filtro.info()

"""# Contar a quantidade de valores faltantes"""

def contar_faltantes_custom(df, colunas=None):
    if colunas is None:
        colunas = df.columns

    faltantes = {}

    for col in colunas:
        faltantes[col] = df[col].apply(lambda x: pd.isna(x) or x in ['', '[]', None]).sum()

    return pd.Series(faltantes).sort_values(ascending=False)
contar_faltantes_custom(df_filtro)

"""Quantidade de dados faltantes por ano"""

def eh_faltante(x):
    return pd.isna(x) or x in ['', '[]', None]

df_filtro['tags_faltando'] = df_filtro['tags'].apply(eh_faltante)
df_filtro['categories_faltando'] = df_filtro['categories'].apply(eh_faltante)

faltantes_por_ano = df_filtro.groupby('year')[['tags_faltando', 'categories_faltando']].sum().reset_index()

plt.figure(figsize=(10, 6))
plt.plot(faltantes_por_ano['year'], faltantes_por_ano['tags_faltando'], label='Tags faltantes', marker='o')
plt.plot(faltantes_por_ano['year'], faltantes_por_ano['categories_faltando'], label='Categories faltantes', marker='o')
plt.title('Quantidade de dados faltantes por ano')
plt.xlabel('Ano')
plt.ylabel('Quantidade de faltantes')
plt.legend()
plt.tight_layout()
plt.show()

"""##Quais são as fontes que têm a variável categoria sem valores?


"""

# Tratar como ausente: NaN, '', '[]'
linhas_sem_categoria = df_filtro[df_filtro['categories'].isin([None, '', '[]']) | df_filtro['categories'].isna()]

# Ver fontes únicas
sources_sem_categoria = linhas_sem_categoria['source_name'].unique()

# Exibir
print("Fontes (source_name) com linhas sem categoria:")
for source in sources_sem_categoria:
    print(f"- {source}")

"""## Quais são essas linhas?"""

df_filtro[df_filtro['source_name'].isin(['ESTADAO_VERIFICA', 'fato-ou-fake', 'lupa']) & df_filtro['categories'].isin([None, '', '[]']) | df_filtro['categories'].isna()]

"""# "Drop" as linhas que têm valores vazios em text_news"""

df_filtro = df_filtro.dropna(subset=['text_news'])
df_filtro = df_filtro.reset_index(drop=True)

"""# Quais são as categorias?"""

print(df_filtro['categories'].unique())

"""Describe:"""

df_filtro.describe(include='all')

"""1. `url`: URLs das notícias (6.600 únicas, alta variabilidade).
2.` source_name: `Nome da fonte/agência (6 categorias distintas, sendo "boatos" a mais frequente, com 3.032 ocorrências
3.` title:` Títulos das notícias (6.653 únicos, alta diversidade).
4. `text_name:` Texto principal (6.997 únicos, quase todos distintos).

4. `categories`:Categorias (25 únicas, com uma categoria dominante em 2.099 ocorrências).

6. `tags:` Tags (3.068 únicas, mas 3.780 valores ausentes).

## Normalização e conversão para lista das categorias tags e categories
"""

def convert_to_list(value):
    if pd.isna(value) or value in ['', '[]']:
        return []
    try:
        # Remove aspas extras e converte string de lista para lista
        return ast.literal_eval(value)
    except (ValueError, SyntaxError):
        # Se não for uma lista válida, trata como valor único
        return [value]

df_filtro['tags'] = df_filtro['tags'].apply(convert_to_list)
df_filtro['categories'] = df_filtro['categories'].apply(convert_to_list)

def normalize_text(text):
    if isinstance(text, list):
        return [normalize_text(item) for item in text]
    # Converter para minúsculas
    text = text.lower()
    # Remover acentos
    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')
    # Remover caracteres especiais (opcional)
    text = re.sub(r'[^a-z0-9\s]', '', text)
    return text.strip()

df_filtro['tags'] = df_filtro['tags'].apply(lambda x: [normalize_text(tag) for tag in x])
df_filtro['categories'] = df_filtro['categories'].apply(lambda x: [normalize_text(cat) for cat in x])

"""# Preenchendo valores vazios"""

df_filtro['tags'] = df_filtro['tags'].apply(lambda x: 'desconhecido' if x == [] else x)

df_filtro['categories'] = df_filtro['categories'].apply(
    lambda x: ["desconhecido"] if (not x or x == ["[]"]) else x
)

df_filtro['categories']

df_filtro['categories'] = df_filtro['categories'].apply(
    lambda x: "desconhecido" if (pd.isna(x) or x == "[]" or x == "") else x
)
df_filtro

df_filtro = df_filtro.drop(columns=['tags_faltando', 'categories_faltando'])

"""# Normalizar a coluna rating"""

def normalizar_lista_rating(item):
    try:
        lista = ast.literal_eval(item) if isinstance(item, str) else item
        if isinstance(lista, list):
            return [x.strip().upper() for x in lista]
        return []
    except:
        return []

df_filtro['rating'] = df_filtro['rating'].apply(normalizar_lista_rating)

df_filtro['rating']

"""# Extrair todos os valores únicos das listas em 'rating'

"""

# Extrair todos os valores únicos das listas em 'rating'
all_ratings = []

for item in df_filtro['rating']:
    try:
        # Converte a string de lista para uma lista real pra auxiliar na contagem de cada valor
        lista = ast.literal_eval(item) if isinstance(item, str) else item
        if isinstance(lista, list):
            all_ratings.extend([r.strip().upper() for r in lista])  # Limpa e normaliza
    except:
        continue  # pula valores com erro

# Contar a frequência de cada valor
rating_counts = Counter(all_ratings)

print("Frequência de valores em 'rating':")
for rating, count in rating_counts.most_common():
    print(f"{rating}: {count}")

"""Mapeamento do rating"""

mapa_rating = {
    # 0 - Totalmente falso
    'FALSO': 0,
    'FAKE': 0,
    'BOATO': 0,

    # 1 - Parcialmente falso ou distorcido
    'ENGANOSO': 1,
    'IMPRECISO': 1,
    'FORA DE CONTEXTO': 1,
    'DISTORCIDO': 1,
    'INSUSTENTÁVEL': 1,
    'EXAGERADO': 1,

    # 2 - Ambíguo
    'NAO E BEM ASSIM': 2,
    'CONTRADITÓRIO': 2,
    'DE OLHO': 2,
    'AINDA É CEDO PARA DIZER': 2,
    'SUBESTIMADO': 2,
    'VERDADEIRO, MAS': 2,



    # 4 - Totalmente verdadeiro
    'VERDADEIRO': 3,
    'FATO': 3,
    'COMPROVADO': 3
}

"""# **Criando coluna rating escala, que é calculada a partir da média média do mapeamento criado acima**"""

def calcular_media_rating(lista):
    valores = [mapa_rating.get(item, None) for item in lista]
    valores = [v for v in valores if v is not None]
    return round(sum(valores) / len(valores), 2) if valores else None


# Aplicando no DataFrame
df_filtro['rating_escala'] = df_filtro['rating'].apply(calcular_media_rating)

df_filtro

"""# 4. Análise descritiva"""

# Verificar estilos disponíveis
print("Estilos disponíveis:", plt.style.available)

# Convertendo a data e criando colunas de ano e mês
df_filtro['publication_date'] = pd.to_datetime(df_filtro['publication_date'])
df_filtro['year'] = df_filtro['publication_date'].dt.year
df_filtro['month'] = df_filtro['publication_date'].dt.month
df_filtro['year_month'] = df_filtro['publication_date'].dt.to_period('M')

# Configurações de estilo - usando um estilo disponível
plt.style.use('ggplot')  # Ou outro estilo da lista disponível
sns.set_style("whitegrid")
sns.set_palette("husl")

# Criando a figura
plt.figure(figsize=(18, 7))

# Gráfico de barras com contagem por mês/ano
counts = df_filtro['year_month'].value_counts().sort_index()
ax = counts.plot(kind='bar', width=0.9)

# Melhorando os rótulos e título
plt.title('Distribuição Temporal de Checagens de Fatos', fontsize=16, pad=20)
plt.xlabel('Ano e Mês', fontsize=12)
plt.ylabel('Número de Checagens', fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)

# Adicionando linhas de grade para melhor leitura
ax.yaxis.grid(True, linestyle='--', alpha=0.7)

# Adicionando valores nas barras
for i, v in enumerate(counts):
    if v > 0:  # Só adiciona texto se o valor for maior que zero
        ax.text(i, v + (0.05*max(counts)), str(v), ha='center', fontsize=9)

# Adicionando uma linha de tendência
ax.plot(counts.index.astype(str), counts.values, color='skyblue', alpha=0.5, marker='o')

# Ajustando layout
plt.tight_layout()
plt.show()

# Padrões sazonais
plt.figure(figsize=(12, 6))
df_filtro['month'].value_counts().sort_index().plot(kind='bar')
plt.title('Distribuição por Mês (todos os anos)')
plt.xticks(range(12), ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun', 'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez'])
plt.show()

# Heatmap ano x mês
cross_tab = pd.crosstab(df_filtro['year'], df_filtro['month'])
plt.figure(figsize=(12, 8))
sns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlOrRd')
plt.title('Heatmap de Checagens: Ano vs Mês')
plt.show()

def safe_convert(x):
    if isinstance(x, list):  # If already a list, return as-is
        return x
    try:
        return ast.literal_eval(x)  # If string like "['x']", convert to list
    except (ValueError, SyntaxError):
        return [x]  # If malformed, wrap in a list

categories = df_filtro['categories'].apply(safe_convert).explode()
categories.value_counts().plot(kind='bar')



# Contar notícias por agência
contagem_agencias = df_filtro['source_name'].value_counts()

# Criar o gráfico de barras
ax = contagem_agencias.plot(kind='bar', color='skyblue', figsize=(10, 6))

# Adicionar os valores em cima das barras
for i, valor in enumerate(contagem_agencias):
    ax.text(i, valor + 0.5, str(valor), ha='center', va='bottom')  # +0.5 para espaçamento

# Personalizar o gráfico
plt.title('Quantidade de Notícias por Agência')
plt.xlabel('Agência')
plt.ylabel('Número de Notícias')
plt.xticks(rotation=45)  # Rotacionar nomes das agências se necessário
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()  # Ajustar layout para evitar cortes
plt.show()

publicacoes_por_ano_agencia = df_filtro.groupby(['year', 'source_name']).size().reset_index(name='count')
plt.figure(figsize=(12, 6))
sns.barplot(
    data=publicacoes_por_ano_agencia,
    x='year',
    y='count',
    hue='source_name',
    palette='viridis'
)

plt.title('Publicações por Agência por Ano')
plt.xlabel('Ano')
plt.ylabel('Número de Publicações')
plt.legend(title='Agência', bbox_to_anchor=(1.05, 1))  # Legenda fora do gráfico
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

def parse_categorical_rating(rating_str):
    try:
        if pd.isna(rating_str):
            return None

        # Handle list format
        if isinstance(rating_str, str) and rating_str.startswith('['):
            return rating_str.strip('[]').split(',')[0].strip().strip("'\"")

        return str(rating_str).strip()
    except:
        return None

df_filtro['rating'] = df_filtro['rating'].apply(parse_categorical_rating)
df_filtro = df_filtro.dropna(subset=['rating'])

# Plot with better formatting
ax = df_filtro['rating'].value_counts().plot(
    kind='bar',
    title='Distribuição das Classificações',
    figsize=(10, 6)
)
ax.set_xlabel('Classificação')
ax.set_ylabel('Contagem')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Contar objetos por ano e ordenar
contagem_por_ano = df_filtro['year'].value_counts().sort_index()

# Plotar o gráfico de barras
ax = contagem_por_ano.plot(kind='bar', color='skyblue', figsize=(10, 6))

# Adicionar os valores em cima das barras
for i, valor in enumerate(contagem_por_ano):
    ax.text(i, valor + 0.2, str(valor), ha='center', va='bottom')  # +0.2 para espaçamento

# Personalizar
plt.title('Quantidade de Publicações por Ano')
plt.xlabel('Ano')
plt.ylabel('Número de Publicações')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Agrupar por ano e contar publicações
publicacoes_por_ano = df_filtro['year'].value_counts().sort_index().reset_index()
publicacoes_por_ano.columns = ['Ano', 'Número de Publicações']

# Gráfico de linha
plt.figure(figsize=(12, 6))
sns.lineplot(
    data=publicacoes_por_ano,
    x='Ano',
    y='Número de Publicações',
    marker='o',
    color='royalblue'
)
plt.title('Evolução do Número de Publicações por Ano')
plt.xlabel('Ano')
plt.ylabel('Número de Publicações')
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

from wordcloud import WordCloud

# Concatenar todos os títulos
texto = ' '.join(df_filtro['text_news'].dropna())

# Gerar nuvem de palavras
wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='viridis'
).generate(texto)

# Plotar
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Palavras Mais Frequentes nos Textos')
plt.show()

from wordcloud import WordCloud

# Concatenar todos os títulos
texto = ' '.join(df_filtro['title'].dropna())

# Gerar nuvem de palavras
wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='viridis'
).generate(texto)

# Plotar
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Palavras Mais Frequentes nos Títulos')
plt.show()

"""## Seleção da amostra"""

proporcoes_ano = df_filtro['year'].value_counts(normalize=True)
print(proporcoes_ano)

proporcoes = df_filtro['source_name'].value_counts(normalize=True)
print(proporcoes)

df_filtro.to_csv('/content/gdrive/MyDrive/dados_filtrado.csv', index=False)

"""## adicionar agência e ano"""

# coluna que combine source_name e year para estratificação conjunta
df_filtro['strata'] = df_filtro['source_name'].astype(str) + '_' + df_filtro['year'].astype(str)

tamanho_amostra = int(0.30 * len(df_filtro))

# amostra estratificada baseada na coluna 'strata'
df_strata, _ = train_test_split(
    df_filtro,
    train_size=tamanho_amostra,
    stratify=df_filtro['strata'],
    random_state=42
)

df_strata = df_strata.drop(columns=['strata'])

"""## **PRE PROCESSAMENTO DO TEXTO**"""

!pip install nltk  # Para processamento de texto

"""## 2. Remoção de palavras e conteúdo desnecessários

"""

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('rslp')  # Para stemmer em português

# Carregar stopwords em português
stop_words = set(stopwords.words('portuguese'))

stop_words.update(["o", "a", "os", "as", "um", "uma", "uns", "umas", "de", "da", "do", "das", "dos"])

!pip install unidecode

from unidecode import unidecode

def preprocessar_texto(texto, aplicar_stemmer=False):
    if not isinstance(texto, str):
        return ""
    texto = texto.lower()
    texto = unidecode(texto)

    # Remover pontuações, números, etc.
    texto = re.sub('\s+', ' ', texto)  # Remove extra spaces
    texto = re.sub('\S*@\S*\s?', '', texto)  # Remove emails
    texto = re.sub('\'', '', texto)  # Remove apostrophes
    texto = re.sub('[^a-zA-Z]', ' ', texto)  # Remove non-alphabet

    texto = texto.lower()

    # Tokenizar
    tokens = word_tokenize(texto, language='portuguese')

    # Remover stopwords
    tokens = [palavra for palavra in tokens if palavra not in stop_words and len(palavra) > 2]

    return " ".join(tokens)

nltk.download('punkt_tab')

# aplique o pré-processamento básico
df_strata['title_limpo'] = df_strata['title'].astype(str).apply(preprocessar_texto)
df_strata['text_news_limpo'] = df_strata['text_news'].astype(str).apply(preprocessar_texto)

from collections import Counter

# Juntar todos os textos em uma única string
texto_completo = ' '.join(df_strata['text_news_limpo'])

# Contar frequência de palavras
palavras = texto_completo.split()
contagem_palavras = Counter(palavras)
top_palavras = contagem_palavras.most_common(20)

# Gráfico de barras
plt.figure(figsize=(12, 6))
sns.barplot(
    x=[count for word, count in top_palavras],
    y=[word for word, count in top_palavras],
    palette='viridis'
)
plt.title('Top 20 Palavras Mais Frequentes')
plt.xlabel('Contagem')
plt.ylabel('Palavra')
plt.tight_layout()
plt.show()

from nltk import ngrams

# Extrair bigramas (pares de palavras)
bigramas = list(ngrams(palavras, 2))
contagem_bigramas = Counter(bigramas).most_common(25)

# Dataframe para visualização
df_bigramas = pd.DataFrame(contagem_bigramas, columns=['Bigrama', 'Contagem'])
df_bigramas['Bigrama'] = df_bigramas['Bigrama'].apply(lambda x: ' '.join(x))

# Gráfico
plt.figure(figsize=(10, 6))
sns.barplot(
    data=df_bigramas,
    y='Bigrama',
    x='Contagem',
    palette='viridis'
)
plt.title('Top 10 Bigramas Mais Frequentes')
plt.tight_layout()
plt.show()

df_bigramas['Bigrama']

df_strata

df_strata.to_csv('/content/gdrive/MyDrive/base_dados_strata.csv', index=False)

"""## 3.Bag of Words

`---> ` técnica de processamento de texto que converte documentos em vetores numéricos, contando a frequência de cada palavra em um texto.
"""

# 1. Reinicie o ambiente (Runtime > Restart runtime) antes de executar!

# 2. Instale as versões compatíveis:
!pip install --upgrade --force-reinstall "numpy<2.0" "gensim==4.3.3" "thinc<8.3.0"

# 3. Verifique se funcionou:
import numpy as np
import gensim

import spacy #carregando bibliotecas e funções de limpeza dos textos
!python -m spacy download pt_core_news_lg

# Carrega o modelo de português
nlp = spacy.load("pt_core_news_lg")

def aplicar_lemmatizacao(texto):
    if not isinstance(texto, str):
        return ""

    doc = nlp(texto)
    # Filtra tokens que não são pontuação ou espaços e lematiza
    tokens_lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]

    return " ".join(tokens_lemmas)

df_strata['text_news_lemma'] = df_strata['text_news_limpo'].apply(aplicar_lemmatizacao)
df_strata['title_lemma'] = df_strata['title_limpo'].apply(aplicar_lemmatizacao)

# Você pode juntar o texto e título lematizados, se quiser:
df_strata['texto_completo_lemma'] = df_strata['title_lemma'] + " " + df_strata['text_news_lemma']

df_strata['text_news_lemma']

standar = ['outro','este','ate','de','fake','ir','sao','ser','tambem','dizer','fazer','caiunarede','falso', 'verificar', 'verificamos', 'boato', 'verifica','fake', 'em','para','estão','que','por']
pdregex = r'\b(?:{})\b'.format('|'.join(map(re.escape,standar)))
df_strata['text_news_lemma_limpo'] = df_strata['text_news_lemma'].str.replace(pdregex, '', regex=True)

df_strata['title_lemma_limpo'] = df_strata['title_lemma'].str.replace(pdregex, '', regex=True)

df_strata['texto_completo'] = df_strata['title_lemma_limpo'] + ' ' + df_strata['text_news_lemma_limpo']

contagem = df_strata['categories'].apply(lambda x: 'desconhecido' in x).value_counts()
print(contagem)

#df_strata = df_strata.drop(columns=['text_news_lemma', 'title_lemma'])

df_strata['text_news_lemma_limpo']

df_strata

print(sorted(list(stop_words))[:20])

# texto
vectorizer_text = CountVectorizer(max_df=0.95, min_df=2)  # Limite para reduzir dimensionalidade
X_dtm_text = vectorizer_text.fit_transform(df_strata['text_news_lemma_limpo'])

#titulo
vectorizer_title = CountVectorizer(max_df=0.95, min_df=2)  # Limite para reduzir dimensionalidade

X_dtm_tittle = vectorizer_title.fit_transform(df_strata['title_lemma_limpo'])

X_dtm_tittle

"""Tentando o LDA...

TEXTO
"""

from wordcloud import WordCloud

# Concatenar todos os títulos (ajuste para sua coluna de texto)
texto = ' '.join(df_strata['text_news_lemma_limpo'].dropna())

# Gerar nuvem de palavras
wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='viridis'
).generate(texto)

# Plotar
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Palavras Mais Frequentes no Texto')
plt.show()

"""##Título"""

# Concatenar todos os títulos (ajuste para sua coluna de texto)
texto = ' '.join(df_strata['title_lemma_limpo'].dropna())

# Gerar nuvem de palavras
wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='viridis'
).generate(texto)

# Plotar
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Palavras Mais Frequentes nos Títulos')
plt.show()

"""## ///////////////

# NOVA TENTATIVA DE LDA - 10/06

referência: https://www.kaggle.com/code/gauravduttakiit/latent-dirichlet-allocation

Copyright [2021]  [Gaurav Dutta]

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

#Título
"""

#titulo
vectorizer_title = CountVectorizer(max_features=5000)  # Limite para reduzir dimensionalidade

X_dtm_tittle = vectorizer_title.fit_transform(df_strata['title_lemma_limpo'])

from sklearn.decomposition import LatentDirichletAllocation

LDA = LatentDirichletAllocation(n_components=12,random_state=42)

LDA.fit(X_dtm_tittle)

len(vectorizer_title.get_feature_names_out())

import random

for i in range(10):
    random_word_id = random.randint(0,1686)
    print(vectorizer_title.get_feature_names_out()[random_word_id])

for i in range(10):
    random_word_id = random.randint(0,1686)
    print(vectorizer_title.get_feature_names_out()[random_word_id])

len(LDA.components_)

LDA.components_

single_topic = LDA.components_[0]

single_topic.argsort()

single_topic[2544]

single_topic[2511]

single_topic.argsort()[-10:]

top_word_indices = single_topic.argsort()[-10:]

for index in top_word_indices:
    print(vectorizer_title.get_feature_names_out()[index])

for index,topic in enumerate(LDA.components_):
    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')
    print([vectorizer_title.get_feature_names_out()[i] for i in topic.argsort()[-15:]])
    print('\n')

X_dtm_tittle.shape

len(df_strata)

topic_results = LDA.transform(X_dtm_tittle)

topic_results.shape

#PRIMEIRO ARTIGO (linha1), COMO VER:
topic_results[0].round(2)
topic_results[0].argmax()
#isso indica que o lda está no 7

df_strata['title_lemma_limpo'].head()

topic_results.argmax(axis=1)

df_strata['titulo_lda'] = topic_results.argmax(axis=1)

df_strata.head()

"""# TEXT NEWS"""

LDA_text = LatentDirichletAllocation(n_components=8,random_state=42)

LDA_text.fit(X_dtm_text)

len(vectorizer_text.get_feature_names_out())

for i in range(10):
    random_word_id = random.randint(0,12866)
    print(vectorizer_text.get_feature_names_out()[random_word_id])

for i in range(10):
    random_word_id_text = random.randint(0,12874)
    print(vectorizer_text.get_feature_names_out()[random_word_id_text])

len(LDA_text.components_)

LDA_text.components_

len(LDA_text.components_[0])

single_topic_text = LDA_text.components_[0]
single_topic_text.argsort()

# Word least representative of this topic
single_topic_text[1706]

# Word most representative of this topic
single_topic_text[3221]

# Top 10 words for this topic:
single_topic_text.argsort()[-10:]

top_word_indices_text = single_topic_text.argsort()[-10:]

for index in top_word_indices_text:
    print(vectorizer_text.get_feature_names_out()[index])

for index,topic in enumerate(LDA_text.components_):
    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')
    print([vectorizer_text.get_feature_names_out()[i] for i in topic.argsort()[-20:]])
    print('\n')

topic_results_text = LDA_text.transform(X_dtm_text)

topic_results_text.argmax(axis=1)
df_strata['texto_lda'] = topic_results_text.argmax(axis=1)

print(topic_results_text.shape)  # Mostra (n_documentos, n_tópicos)

"""////////////

## ////////////////////////////////////////////

## K-means - título
"""

topic_results

from sklearn.cluster import KMeans

# Definir número de clusters (k) baseado na análise exploratória
kmeans = KMeans(n_clusters=3, random_state=100).fit(topic_results)  # Teste k entre 5-12
df_strata['cluster'] = kmeans.labels_

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
doc_topics_scaled = scaler.fit_transform(topic_results)

from sklearn.metrics import silhouette_score
print(f"Silhouette Score: {silhouette_score(topic_results, kmeans.labels_):.4f}")

from sklearn.metrics import silhouette_score, davies_bouldin_score

# 3. Determinar número ótimo de clusters com Elbow Method
wcss = []
silhouette_scores = []
db_scores = []
k_range = range(3, 12)  # Testando de 3 a 11 clusters

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=100, n_init=10)
    kmeans.fit(doc_topics_scaled)
    wcss.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(doc_topics_scaled, kmeans.labels_))
    db_scores.append(davies_bouldin_score(doc_topics_scaled, kmeans.labels_))

# Plotar gráficos para ajudar na decisão
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(k_range, wcss, 'bo-')
plt.xlabel('Número de clusters')
plt.ylabel('WCSS')
plt.title('Método Elbow')

plt.subplot(1, 3, 2)
plt.plot(k_range, silhouette_scores, 'go-')
plt.xlabel('Número de clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score')

plt.subplot(1, 3, 3)
plt.plot(k_range, db_scores, 'ro-')
plt.xlabel('Número de clusters')
plt.ylabel('Davies-Bouldin Score')
plt.title('Davies-Bouldin Score')

plt.tight_layout()
plt.show()

# 4. Escolher o melhor k baseado nas métricas (exemplo: máximo silhouette)
optimal_k = k_range[np.argmax(silhouette_scores)]
print(f"\nNúmero ótimo de clusters sugerido: {optimal_k}")

"""# Kmeans com LDA"""

# Definir número de clusters (k) baseado na análise exploratória
kmeans_text = KMeans(n_clusters=4, random_state=100).fit(topic_results_text)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
doc_topics_scaled_text = scaler.fit_transform(topic_results_text)

df_strata['cluster_text'] = kmeans_text.labels_

from sklearn.metrics import silhouette_score
print(f"Silhouette Score: {silhouette_score(topic_results_text, kmeans_text.labels_):.4f}")

# Criar DataFrame com as distribuições de tópicos
doc_topics_scaled_text = pd.DataFrame(topic_results_text)

# Calcular a média de cada tópico por cluster
cluster_topic_dist = doc_topics_scaled_text.groupby(df_strata['cluster_text']).mean()

# Mostrar a distribuição completa
print("\nDistribuição média de tópicos por cluster:")
print(cluster_topic_dist.round(3))

# Agora vamos mostrar os tópicos mais relevantes para cada cluster
print("\nTópicos dominantes por cluster:")
for cluster in range(kmeans_text.n_clusters):
    print(f"\nCluster {cluster}:")

    # Pegar os tópicos mais relevantes para este cluster (ordenados por importância)
    sorted_topics = cluster_topic_dist.loc[cluster].sort_values(ascending=False)

    # Mostrar os top 3 tópicos e suas palavras-chave
    for topic_idx, topic_weight in sorted_topics.head(8).items():
        print(f"  Tópico {topic_idx} (peso: {topic_weight:.3f}):")

        # Pegar as palavras mais relevantes para este tópico
        top_words = [vectorizer_title.get_feature_names_out()[i]
                    for i in LDA.components_[topic_idx].argsort()[-10:][::-1]]

        print(f"    Palavras-chave: {', '.join(top_words)}")

inertias = []
k_values = range(2, 11)  # Testando de 2 a 10 clusters

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(topic_results_text)
    inertias.append(kmeans.inertia_)

# Plotando o gráfico do cotovelo
plt.figure(figsize=(8, 5))
plt.plot(k_values, inertias, 'bo-')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Inércia')
plt.title('Método do Cotovelo')
plt.grid()
plt.show()

silhouette_scores = []
k_values = range(2, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(topic_results_text)
    score = silhouette_score(topic_results_text, kmeans.labels_)
    silhouette_scores.append(score)

plt.figure(figsize=(8, 5))
plt.plot(k_values, silhouette_scores, 'go-')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score')
plt.grid()
plt.show()

plt.figure(figsize=(12, 6))
sns.heatmap(
    cluster_topic_dist,
    annot=True,
    fmt=".3f",
    cmap='Oranges',
    linewidths=0.5,
    annot_kws={"size": 10},
    cbar_kws={'label': 'Proporção Média de Associação ao Tópico'}
)
plt.title("Distribuição Média dos Tópicos por Cluster", fontsize=14, pad=15)
plt.xlabel("Tópico - LDA", fontsize=12)
plt.ylabel("Cluster -Kmeans", fontsize=12)
plt.xticks(rotation=0)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

print(df_strata.columns)

fact_analise = df_strata[['source_name', 'title', 'publication_date', 'text_news', 'categories', 'tags', 'year', 'rating_escala','month', 'year_month', 'titulo_lda', 'texto_lda', 'cluster', 'cluster_text']]

fact_analise

fact_analise.to_csv('/content/gdrive/MyDrive/fact_analise.csv', index=False)

fact_analise_test = df_strata[['text_news', 'texto_lda','cluster_text']]

fact_analise_test .to_csv('/content/gdrive/MyDrive/fact_analise_test.csv', index=False)